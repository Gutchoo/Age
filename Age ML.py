# -*- coding: utf-8 -*-
"""FINAL VERSION - IEOR142 FINAL PROJECT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HLRKuy2FxqciBNxqqEtFO7IyJTC_rhRT
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

import os
charles_path = '/content/drive/MyDrive/Berkeley Spring 2023/Indeng 142'
os.chdir(charles_path)  # Change working directory to new_path

from sklearn.tree import plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
import statsmodels.formula.api as smf
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from statsmodels.stats.outliers_influence import variance_inflation_factor
from scipy.stats.mstats import winsorize

"""## 1. Loading Datasets"""

# original dataset from Kaggle https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who?resource=download

life_exp = pd.read_csv('Life Expectancy Data.csv')

# dataset obtained from this website https://data.worldbank.org/indicator/SP.DYN.TFRT.IN?most_recent_year_desc=true
fertility = pd.read_csv("fertility_rate.csv")

# dataset obtained from https://data.worldbank.org/indicator/SP.POP.TOTL?end=2015&start=2000&view=chart
pop_size = pd.read_csv("population_size.csv")

# dataset obtained from https://data.worldbank.org/indicator/NY.GDP.PCAP.PP.CD?name_desc=false&view=chart
GDP_per_capita = pd.read_csv("GDP_Per_Capita_PPP_Current_International_Dollar.csv")

# dataset obtained from https://data.worldbank.org/indicator/IE.PPI.WATR.CD?end=2021&name_desc=false&start=2021&view=bar
Investment_in_water_and_sanitation = pd.read_csv("Investment_in_water_and_sanitation_with_private_participation_current_US_Dollar.csv")

# dataset obtained from https://data.worldbank.org/indicator/SI.POV.DDAY?view=chart
Poverty_headcount = pd.read_csv("Poverty_headcount_ratio_at_2_15_a_day_2017_PPP_percent_of_population.csv")

# dataset obtained from https://data.worldbank.org/indicator/SH.DTH.COMM.ZS?end=2019&start=2019&view=map
Cause_of_death_commuunicable = pd.read_csv("Cause_of_death_by_communicable_diseases_and_maternal_prenatal_and_nutrition_conditions_percent_of_total.csv")

# dataset obtained from https://data.worldbank.org/indicator/SH.DTH.NCOM.ZS?view=map
Cause_of_death_non_communicable = pd.read_csv("Cause_of_death_by_non_communicable_diseases_percent_of_total.csv")

# dataset obtained from https://data.worldbank.org/indicator/SP.DYN.IMRT.IN?view=chart
Infant_Mortality_rate = pd.read_csv("Mortality_rate_infant_per_1000_live_births.csv")

# dataset obtained from https://data.worldbank.org/indicator/SG.DMK.SRCR.FN.ZS?end=2020&start=2020&view=bar
Women_informed_sexual_relation_decisions = pd.read_csv("Women_making_their_own_informed_decisions_regarding_sexual_relations_contraceptive_use_and_reproductive_health_care_percent_of_women_age_15_to_49.csv")

"""## 2. Data Cleaning and Merging Into One Dataset"""

#dropping columns that are not useful for our purposes
fertility.drop(['Indicator Name', 'Indicator Code'], axis = 1, inplace = True)

pop_size.drop(['Indicator Name', 'Indicator Code'], axis = 1, inplace = True)

GDP_per_capita.drop(['Indicator Name', 'Indicator Code'], axis = 1, inplace = True)

Investment_in_water_and_sanitation.drop(['Indicator Name', 'Indicator Code'], axis = 1, inplace = True)

Poverty_headcount.drop(['Indicator Name', 'Indicator Code'], axis = 1, inplace = True)

Cause_of_death_commuunicable.drop(['Indicator Name', 'Indicator Code'], axis = 1, inplace = True)

Cause_of_death_non_communicable.drop(['Indicator Name', 'Indicator Code'], axis = 1, inplace = True)

Infant_Mortality_rate.drop(['Indicator Name', 'Indicator Code'], axis = 1, inplace = True)

Women_informed_sexual_relation_decisions.drop(['Indicator Name', 'Indicator Code'], axis = 1, inplace = True)

#inverting the dataframes from columns to rows so we can merge with the life_exp dataset
fertility_inverted = fertility.melt(id_vars = ['Country Name', 'Country Code'],
                                  value_vars = ['2000', '2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015'],
                                  var_name = 'Year',
                                  value_name = 'fertility_rate')

pop_size_inverted = pop_size.melt(id_vars = ['Country Name', 'Country Code'],
                                  value_vars = ['2000', '2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015'],
                                  var_name = 'Year',
                                  value_name = 'Population_size')

GDP_per_capita_inverted = GDP_per_capita.melt(id_vars = ['Country Name', 'Country Code'],
                                  value_vars = ['2000', '2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015'],
                                  var_name = 'Year',
                                  value_name = 'GDP_per_capita')

Investment_in_water_and_sanitation_inverted = Investment_in_water_and_sanitation.melt(id_vars = ['Country Name', 'Country Code'],
                                  value_vars = ['2000', '2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015'],
                                  var_name = 'Year',
                                  value_name = 'utility_infrastructure')

Poverty_headcount_inverted = Poverty_headcount.melt(id_vars = ['Country Name', 'Country Code'],
                                  value_vars = ['2000', '2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015'],
                                  var_name = 'Year',
                                  value_name = 'Poverty')

Cause_of_death_commuunicable_inverted = Cause_of_death_commuunicable.melt(id_vars = ['Country Name', 'Country Code'],
                                  value_vars = ['2000', '2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015'],
                                  var_name = 'Year',
                                  value_name = 'communicable_disease')

Cause_of_death_non_communicable_inverted = Cause_of_death_non_communicable.melt(id_vars = ['Country Name', 'Country Code'],
                                  value_vars = ['2000', '2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015'],
                                  var_name = 'Year',
                                  value_name = 'non_communicable_disease')

Infant_Mortality_rate_inverted = Infant_Mortality_rate.melt(id_vars = ['Country Name', 'Country Code'],
                                  value_vars = ['2000', '2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015'],
                                  var_name = 'Year',
                                  value_name = 'Infant_mortality')

Women_informed_sexual_relation_decisions_inverted = Women_informed_sexual_relation_decisions.melt(id_vars = ['Country Name', 'Country Code'],
                                  value_vars = ['2000', '2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015'],
                                  var_name = 'Year',
                                  value_name = 'Women_Information')

# turning the 'Year' variable into an int type to be the same type as 'Year' in life_exp dataframe and we can merge
fertility_inverted = fertility_inverted.assign(Year = fertility_inverted['Year'].astype(int))
# changing 'Country Name' to 'Country' so that it has the same variable name as in the life_exp dataframe and we can merge on that
fertility_inverted = fertility_inverted.assign(Country = fertility_inverted['Country Name'])
fertility_inverted.drop('Country Name', axis = 1, inplace=True)

# turning the 'Year' variable into an int type to be the same type as 'Year' in life_exp dataframe and we can merge
pop_size_inverted = pop_size_inverted.assign(Year = pop_size_inverted['Year'].astype(int))
# changing 'Country Name' to 'Country' so that it has the same variable name as in the life_exp dataframe and we can merge on that
pop_size_inverted = pop_size_inverted.assign(Country = pop_size_inverted['Country Name'])
pop_size_inverted.drop('Country Name', axis = 1, inplace=True)

# turning the 'Year' variable into an int type to be the same type as 'Year' in life_exp dataframe and we can merge
GDP_per_capita_inverted = GDP_per_capita_inverted.assign(Year = GDP_per_capita_inverted['Year'].astype(int))
# changing 'Country Name' to 'Country' so that it has the same variable name as in the life_exp dataframe and we can merge on that
GDP_per_capita_inverted = GDP_per_capita_inverted.assign(Country = GDP_per_capita_inverted['Country Name'])
GDP_per_capita_inverted.drop('Country Name', axis = 1, inplace=True)

# turning the 'Year' variable into an int type to be the same type as 'Year' in life_exp dataframe and we can merge
Investment_in_water_and_sanitation_inverted = Investment_in_water_and_sanitation_inverted.assign(Year = Investment_in_water_and_sanitation_inverted['Year'].astype(int))
# changing 'Country Name' to 'Country' so that it has the same variable name as in the life_exp dataframe and we can merge on that
Investment_in_water_and_sanitation_inverted = Investment_in_water_and_sanitation_inverted.assign(Country = Investment_in_water_and_sanitation_inverted['Country Name'])
Investment_in_water_and_sanitation_inverted.drop('Country Name', axis = 1, inplace=True)

# turning the 'Year' variable into an int type to be the same type as 'Year' in life_exp dataframe and we can merge
Poverty_headcount_inverted = Poverty_headcount_inverted.assign(Year = Poverty_headcount_inverted['Year'].astype(int))
# changing 'Country Name' to 'Country' so that it has the same variable name as in the life_exp dataframe and we can merge on that
Poverty_headcount_inverted = Poverty_headcount_inverted.assign(Country = Poverty_headcount_inverted['Country Name'])
Poverty_headcount_inverted.drop('Country Name', axis = 1, inplace=True)

# turning the 'Year' variable into an int type to be the same type as 'Year' in life_exp dataframe and we can merge
Cause_of_death_commuunicable_inverted = Cause_of_death_commuunicable_inverted.assign(Year = Cause_of_death_commuunicable_inverted['Year'].astype(int))
# changing 'Country Name' to 'Country' so that it has the same variable name as in the life_exp dataframe and we can merge on that
Cause_of_death_commuunicable_inverted = Cause_of_death_commuunicable_inverted.assign(Country = Cause_of_death_commuunicable_inverted['Country Name'])
Cause_of_death_commuunicable_inverted.drop('Country Name', axis = 1, inplace=True)

# turning the 'Year' variable into an int type to be the same type as 'Year' in life_exp dataframe and we can merge
Cause_of_death_non_communicable_inverted = Cause_of_death_non_communicable_inverted.assign(Year = Cause_of_death_non_communicable_inverted['Year'].astype(int))
# changing 'Country Name' to 'Country' so that it has the same variable name as in the life_exp dataframe and we can merge on that
Cause_of_death_non_communicable_inverted = Cause_of_death_non_communicable_inverted.assign(Country = Cause_of_death_non_communicable_inverted['Country Name'])
Cause_of_death_non_communicable_inverted.drop('Country Name', axis = 1, inplace=True)

# turning the 'Year' variable into an int type to be the same type as 'Year' in life_exp dataframe and we can merge
Infant_Mortality_rate_inverted = Infant_Mortality_rate_inverted.assign(Year = Infant_Mortality_rate_inverted['Year'].astype(int))
# changing 'Country Name' to 'Country' so that it has the same variable name as in the life_exp dataframe and we can merge on that
Infant_Mortality_rate_inverted = Infant_Mortality_rate_inverted.assign(Country = Infant_Mortality_rate_inverted['Country Name'])
Infant_Mortality_rate_inverted.drop('Country Name', axis = 1, inplace=True)

# turning the 'Year' variable into an int type to be the same type as 'Year' in life_exp dataframe and we can merge
Women_informed_sexual_relation_decisions_inverted = Women_informed_sexual_relation_decisions_inverted.assign(Year = Women_informed_sexual_relation_decisions_inverted['Year'].astype(int))
# changing 'Country Name' to 'Country' so that it has the same variable name as in the life_exp dataframe and we can merge on that
Women_informed_sexual_relation_decisions_inverted = Women_informed_sexual_relation_decisions_inverted.assign(Country = Women_informed_sexual_relation_decisions_inverted['Country Name'])
Women_informed_sexual_relation_decisions_inverted.drop('Country Name', axis = 1, inplace=True)

merged = pd.merge(life_exp,fertility_inverted, on = ['Country', 'Year'], how = 'left') #merging life_exp and fertility_inverted dataset
merged = pd.merge(merged,pop_size_inverted, on = ['Country', 'Year'], how = 'left') #merging life_exp and fertility_inverted dataset
merged = pd.merge(merged,GDP_per_capita_inverted, on = ['Country', 'Year'], how = 'left') #merging merged and GDP_per_capita_inverted dataset
merged = pd.merge(merged,Investment_in_water_and_sanitation_inverted, on = ['Country', 'Year'], how = 'left') #merging merged and Investment_in_water_and_sanitation_inverted dataset
merged = pd.merge(merged,Poverty_headcount_inverted, on = ['Country', 'Year'], how = 'left') #merging merged and Poverty_headcount_inverted dataset
merged = pd.merge(merged,Cause_of_death_commuunicable_inverted, on = ['Country', 'Year'], how = 'left') #merging merged and Cause_of_death_commuunicable_inverted dataset
merged = pd.merge(merged,Cause_of_death_non_communicable_inverted, on = ['Country', 'Year'], how = 'left') #merging merged and Cause_of_death_non_communicable_inverted dataset
merged = pd.merge(merged,Infant_Mortality_rate_inverted, on = ['Country', 'Year'], how = 'left') #merging merged and Infant_Mortality_rate_inverted dataset

merged_all = pd.merge(merged, Women_informed_sexual_relation_decisions_inverted, on = ['Country', 'Year'], how = 'left') #merging merged (all inverted) dataset with Women_informed_sexual_relation_decisions_inverted
merged_all.drop(['Country Code_x', 'Country Code_y', 'Population'], axis = 1, inplace=True)   #'fertility_rate_x','fertility_rate_y','Population Size_x','Population Size_y',

"""## 3. Identifying NaN values"""

# create boolean mask of NaN values
nan_mask = merged_all.isna() #returns True for values that is NaN

# filter dataframe using boolean mask
nan_rows = merged_all[nan_mask.any(axis=1)] #returns only the rows of merged_all where there is an NaN value in the rows

merged_all.isin([1766.947595]).sum()

merged_all.info()

"""## 4. Creating visualizations of distributions of the variables to appropriately replace NaN values"""

import seaborn as sns
numeric_columns = merged_all.select_dtypes(include = ['number']).columns
# Assuming your DataFrame is named 'merged_all'
for column in numeric_columns:
    # Filter the current column for values >= 0
    x = merged_all[column][merged_all[column] >= 0]

    plt.figure(figsize=(10, 5))
    sns.histplot(x, kde=True)

    mean = x.mean()
    median = x.median()
    mode = x.mode()[0]

    plt.axvline(mean, color='r', linestyle='--', label='Mean')
    plt.axvline(median, color='g', linestyle='-', label='Median')
    plt.axvline(mode, color='b', linestyle='-', label='Mode')

    # Add text labels for mean, median, and mode
    plt.text(mean, plt.ylim()[1]*0.9, f"Mean: {mean:.2f}", color='r', fontsize=12)
    plt.text(median, plt.ylim()[1]*0.8, f"Median: {median:.2f}", color='g', fontsize=12)
    plt.text(mode, plt.ylim()[1]*0.7, f"Mode: {mode:.2f}", color='b', fontsize=12)

    plt.legend()
    plt.title(f'Histogram for {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.show()

merged_all.drop(['Country Code'], axis = 1, inplace = True)

"""## 5. Replacing NaN Values"""

columns_to_replace_with_mean = {'Women_information', 'non_communicable_disease', 'Population_size', 'compositioon of resources'}
columns_to_replace_with_median = {'Infant_mortality', 'communicable_disease', 'Poverty', 'utility_infrastructure',  'GDP_per_capita', 'fertility_rate', 'Schooling', 'thinness 5-9 years', 'thinness 1-19 years', 'GDP'}


#numeric_columns = merged_all.select_dtypes(include=['number']).columns

for column in merged_all:
    #replace missing values with the mean or the median
    #after inspecting the histograms.
    if column in columns_to_replace_with_mean:
        mean_value = merged_all[column].mean()
        # replace all NaN values with None
        merged_all.replace(to_replace=['nan', 'NaN', np.nan, None], value=mean_value, inplace=True)
        #merged_all[column].fillna(mean_value, inplace=True)
    elif column in columns_to_replace_with_median:
        median_value = merged_all[column].median()
        merged_all.replace(to_replace=['nan', 'NaN', np.nan, None], value=median_value, inplace=True)
        #merged_all[column].fillna(median_value, inplace=True)

#to export a new final csv file
merged_all.to_csv('merged_output.csv', index=False)

merged_all.columns

#merged_all.isna().sum()
#merged_all.shape
# Select the column and convert the values to a list
column_values = merged_all['Women_Information'].tolist()

# Print the values in the column
print(column_values)

X = merged_all.drop(['Life expectancy ', 'Country'], axis = 1) #dropping 'Country' since that is not a predictive variable and we will get 100+ dummy variables
X = pd.get_dummies(X) #for the only categorical variable "Status"
y = merged_all['Life expectancy ']

X.columns

"""## 6. Calculating VIF"""

# calculate Variance Inflation Factor
from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_scores = pd.DataFrame()
vif_scores["Attribute"] = X.columns

# calculating VIF for each feature
vif_scores["VIF Scores"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]
vif_scores

from statsmodels.stats.outliers_influence import variance_inflation_factor

# calculate VIF scores for each feature in the DataFrame
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif["features"] = X.columns

# subset the DataFrame to include only features with VIF scores > 10
#print(vif[vif["VIF Factor"] > 5])
#print(vif[vif["VIF Factor"] <= 10])

# calculate the correlation matrix
corr_matrix = merged_all.corrwith(merged_all['Life expectancy '])

# subset the correlation matrix to include only highly correlated features
highly_corr_features = corr_matrix[corr_matrix.abs() > 0.1].index.tolist()

# print the highly correlated features
print(highly_corr_features)

"""## 7. Heatmap of Correlation Matrix"""

import matplotlib.pyplot as plt
import seaborn as sns

cor_matrix = merged_all.corr()
fig = plt.figure(figsize=(25,10))
sns.heatmap(cor_matrix,annot=True)

"""## Baseline Model"""

merged_baseline = pd.read_csv('merged_output.csv')

# Load the data
data = merged_baseline

# Filter the training and testing data
train_data = data[data['Year'] < 2013]
test_data = data[data['Year'] > 2012]

# Calculate the mean life expectancy for the training data
mean_life_expectancy = train_data['Life expectancy '].mean()

# Create the predictions for the testing data using the mean life expectancy
predictions = np.full(len(test_data), mean_life_expectancy)

# Calculate the accuracy of the predictions using a confusion matrix
true_labels = test_data['Life expectancy '].values
predicted_labels = predictions


mse = mean_squared_error(true_labels, predicted_labels)

print(f"MSE: {mse}")

"""


## 8. Linear Regression Model"""

merged_output_linear = pd.read_csv('merged_output.csv')

merged_output_linear = merged_output_linear.rename({'Adult Mortality':'adult_mortality', 'Life expectancy ':'life_expectancy', 'infant deaths':'infant_deaths','percentage expenditure': 'percentage_expenditure', 'Hepatitis B': 'Hepatitis_B', ' BMI ': 'BMI', 'under-five deaths ': 'under_five_deaths', 'Total expenditure': 'total_expenditure', 'Diphtheria ':'diphtheria', 'Measles ': 'measles',' HIV/AIDS':'HIV_and_AIDS', ' thinness  1-19 years':'thinness_first_19', ' thinness 5-9 years': 'thinness_five_to_nine', 'Income composition of resources':'income_composition'}, axis='columns')

merged_output_linear.columns

merged_output_linear = merged_output_linear.rename(str.lower, axis='columns')

merged_output_linear.columns

merged_train = merged_output_linear[merged_output_linear['year'] < 2013]
merged_test = merged_output_linear[merged_output_linear['year'] > 2012]

import statsmodels.formula.api as smf

merged_train.columns

ols = smf.ols(formula='life_expectancy ~ adult_mortality + infant_deaths + alcohol + percentage_expenditure \
                       + hepatitis_b + bmi + under_five_deaths + polio + total_expenditure + diphtheria \
                       + measles + hiv_and_aids + gdp + thinness_first_19 + thinness_five_to_nine + income_composition \
                      + schooling + fertility_rate + population_size + gdp_per_capita + utility_infrastructure + poverty \
                      + communicable_disease + non_communicable_disease + infant_mortality + women_information', data = merged_train)

linmod = ols.fit()
print(linmod.summary())

from statsmodels.stats.outliers_influence import variance_inflation_factor

VIF_compatible = merged_output_linear.drop(columns = ['country', 'status', 'year'])
vif = pd.DataFrame()
vif['Features'] = VIF_compatible.columns
vif['VIF'] = [variance_inflation_factor(VIF_compatible.values, i) for i in range(VIF_compatible.shape[1])]
vif

VIF_compatible.drop(columns = ['thinness_first_19'], inplace = True)

vif = pd.DataFrame()
vif['Features'] = VIF_compatible.columns
vif['VIF'] = [variance_inflation_factor(VIF_compatible.values, i) for i in range(VIF_compatible.shape[1])]
vif

VIF_compatible.drop(columns = ['non_communicable_disease'], inplace = True)

vif = pd.DataFrame()
vif['Features'] = VIF_compatible.columns
vif['VIF'] = [variance_inflation_factor(VIF_compatible.values, i) for i in range(VIF_compatible.shape[1])]
vif

VIF_compatible.drop(columns = ['infant_deaths'], inplace = True)

vif = pd.DataFrame()
vif['Features'] = VIF_compatible.columns
vif['VIF'] = [variance_inflation_factor(VIF_compatible.values, i) for i in range(VIF_compatible.shape[1])]
vif

VIF_compatible.drop(columns = ['infant_mortality'], inplace = True)

vif = pd.DataFrame()
vif['Features'] = VIF_compatible.columns
vif['VIF'] = [variance_inflation_factor(VIF_compatible.values, i) for i in range(VIF_compatible.shape[1])]
vif

VIF_compatible.drop(columns = ['bmi'], inplace = True)

vif = pd.DataFrame()
vif['Features'] = VIF_compatible.columns
vif['VIF'] = [variance_inflation_factor(VIF_compatible.values, i) for i in range(VIF_compatible.shape[1])]
vif

VIF_compatible.drop(columns = ['polio'], inplace = True)

vif = pd.DataFrame()
vif['Features'] = VIF_compatible.columns
vif['VIF'] = [variance_inflation_factor(VIF_compatible.values, i) for i in range(VIF_compatible.shape[1])]
vif

VIF_compatible.drop(columns = ['income_composition'], inplace = True)

vif = pd.DataFrame()
vif['Features'] = VIF_compatible.columns
vif['VIF'] = [variance_inflation_factor(VIF_compatible.values, i) for i in range(VIF_compatible.shape[1])]
vif

VIF_compatible.drop(columns = ['gdp'], inplace = True)

vif = pd.DataFrame()
vif['Features'] = VIF_compatible.columns
vif['VIF'] = [variance_inflation_factor(VIF_compatible.values, i) for i in range(VIF_compatible.shape[1])]
vif

VIF_compatible.drop(columns = ['women_information'], inplace = True)

vif = pd.DataFrame()
vif['Features'] = VIF_compatible.columns
vif['VIF'] = [variance_inflation_factor(VIF_compatible.values, i) for i in range(VIF_compatible.shape[1])]
vif

ols = smf.ols(formula='life_expectancy ~ adult_mortality + alcohol + percentage_expenditure + hepatitis_b\
                       + under_five_deaths + measles + total_expenditure + diphtheria + hiv_and_aids +  thinness_five_to_nine	+ schooling +	fertility_rate\
                        + population_size + gdp_per_capita + utility_infrastructure + poverty	+ communicable_disease', data=merged_train)

linmodupdated = ols.fit()
print(linmodupdated.summary())

"""We iteratively removed variables with high(>10) VIF to avoid multicolinearity. We also removed features with low significance: alcohol, utility_infrastructure and communicable_disease."""

ols = smf.ols(formula='life_expectancy ~ adult_mortality + percentage_expenditure + hepatitis_b\
                       + under_five_deaths + measles + total_expenditure + diphtheria + hiv_and_aids +  thinness_five_to_nine	+ schooling +	fertility_rate\
                        + population_size + gdp_per_capita + poverty', data=merged_train)

linmodfinal = ols.fit()
print(linmodfinal.summary())

y_pred_linear = pd.Series(linmodfinal.predict(merged_test))

y_pred_linear.head()

y_pred_linear_rounded = y_pred_linear.apply(np.around)
y_pred_linear_rounded.head()

y_test_linear = merged_test['life_expectancy']
y_test_linear_rounded = y_test_linear.apply(np.around)
y_test_linear_rounded.head()

# Mean Squared Error (MSE)
mse_linear = mean_squared_error(y_test_linear, y_pred_linear)
print("Mean Squared Error:", mse_linear)

# Mean Absolute Error (MAE)
mae_linear = mean_absolute_error(y_test_linear, y_pred_linear)
print("Mean Absolute Error:", mae_linear)

# R-squared score
r2_linear = r2_score(y_test_linear, y_pred_linear)
print("R-squared score:", r2_linear)

from sklearn.metrics import accuracy_score
accuracy_linear = accuracy_score(y_test_linear_rounded, y_pred_linear_rounded)


print(f"Accuracy: {accuracy_linear}")

"""## 9. Logistic Regression Model"""

merged_logistic = pd.read_csv('merged_output.csv')

X = merged_logistic.drop(['Life expectancy ', 'Country'], axis = 1) #dropping 'Country' since that is not a predictive variable and we will get 100+ dummy variables
X = pd.get_dummies(X) #for the only categorical variable "Status"
y = merged_logistic['Life expectancy ']

# We change the column names to word_word, set a column High_Life_Expectancy for logistic classification based on below or above median.
from sklearn.model_selection import train_test_split

X.columns = ['_'.join(column.split()) for column in X.columns]

X['High_Life_Expectancy'] = (y > y.median()).astype(int)
X_train, X_test = train_test_split(X, test_size=0.2, random_state=88)
X_train

X_train.columns

"""We used temp_X.drop to drop features with VIF values greater than 10."""

temp_X = X_train
temp_X = temp_X.drop(['Status_Developed','Status_Developing','High_Life_Expectancy','thinness_1-19_years', 'thinness_5-9_years','communicable_disease', 'non_communicable_disease', 'infant_deaths','Infant_mortality','Polio','Diphtheria','Income_composition_of_resources', 'Schooling','Year','Women_Information','GDP','Alcohol'],axis=1)
vif_edited = pd.DataFrame(data=[variance_inflation_factor(temp_X.values, i) for i in range(len(temp_X.columns))],index=temp_X.columns)
vif_edited

X_train['HIV_AIDS'] = X_train['HIV/AIDS']
X_train = X_train.drop(['HIV/AIDS'],axis=1)
X_train['under_five_deaths'] = X_train['under-five_deaths']
X_train = X_train.drop(['under-five_deaths'],axis=1)
X_train.columns

X_test['HIV_AIDS'] = X_test['HIV/AIDS']
X_test = X_test.drop(['HIV/AIDS'],axis=1)
X_test['under_five_deaths'] = X_test['under-five_deaths']
X_test = X_test.drop(['under-five_deaths'],axis=1)
X_test.columns

import statsmodels.formula.api as smf

logreg = smf.logit(formula = 'High_Life_Expectancy ~ Adult_Mortality + percentage_expenditure + Hepatitis_B + Measles + BMI + under_five_deaths + Total_expenditure + HIV_AIDS + fertility_rate + Population_size + GDP_per_capita + utility_infrastructure + Poverty',
                   data = X_train).fit()

print(logreg.summary())

logreg = smf.logit(formula = 'High_Life_Expectancy ~ Adult_Mortality + percentage_expenditure + Hepatitis_B + BMI + under_five_deaths + Total_expenditure + HIV_AIDS + fertility_rate + Population_size + GDP_per_capita + utility_infrastructure + Poverty',
                   data = X_train).fit()

print(logreg.summary())

logreg = smf.logit(formula = 'High_Life_Expectancy ~ Adult_Mortality + percentage_expenditure + Hepatitis_B + BMI + under_five_deaths + HIV_AIDS + fertility_rate + Population_size + GDP_per_capita + utility_infrastructure + Poverty',
                   data = X_train).fit()

print(logreg.summary())

"""Removed features with insignificant p-values till we get to this final model."""

logreg = smf.logit(formula = 'High_Life_Expectancy ~ Adult_Mortality + percentage_expenditure + Hepatitis_B + under_five_deaths + HIV_AIDS + fertility_rate + Population_size + GDP_per_capita + utility_infrastructure + Poverty',
                   data = X_train).fit()

print(logreg.summary())

from sklearn.metrics import confusion_matrix


y_prob = logreg.predict(X_test)
y_pred = pd.Series([1 if x > 0.5 else 0 for x in y_prob], index=y_prob.index)
y_pred
cm = confusion_matrix(X_test['High_Life_Expectancy'], y_pred)
print ("Confusion Matrix : \n", cm)
print("Accuracy: \n",(cm.ravel()[0]+cm.ravel()[3])/sum(cm.ravel()))

"""

## 10. **CART Model**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
import statsmodels.formula.api as smf
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from statsmodels.stats.outliers_influence import variance_inflation_factor
from scipy.stats.mstats import winsorize

merged_output_cart = pd.read_csv('merged_output.csv')

merged_output_cart.head()

"""In order to calculate VIF values for the data we have to drop the categorical variables since VIF only works on numerical variables. Although CART models are generally less sensitive to multicollinearity than linear models we are still going to address extreme multicollinearity(VIF > 100) just to simplify this model. We start by removing the features with the highest VIF values."""

VIF_compatible = merged_output_cart.drop(columns = ['Country', 'Status', 'Year'])

# Compute VIF values
vif = pd.DataFrame()
vif['Features'] = VIF_compatible.columns
vif['VIF'] = [variance_inflation_factor(VIF_compatible.values, i) for i in range(VIF_compatible.shape[1])]
# Display the VIF values for each feature
print(vif)

VIF_compatible.drop(columns = [' thinness  1-19 years'], inplace = True)

# Compute VIF values
vif = pd.DataFrame()
vif['Features'] = VIF_compatible.columns
vif['VIF'] = [variance_inflation_factor(VIF_compatible.values, i) for i in range(VIF_compatible.shape[1])]
# Display the VIF values for each feature
print(vif)

VIF_compatible.drop(columns = ['non_communicable_disease'], inplace = True)

# Compute VIF values
vif = pd.DataFrame()
vif['Features'] = VIF_compatible.columns
vif['VIF'] = [variance_inflation_factor(VIF_compatible.values, i) for i in range(VIF_compatible.shape[1])]
# Display the VIF values for each feature
print(vif)

VIF_compatible.drop(columns = ['infant deaths'], inplace = True)

# Compute VIF values
vif = pd.DataFrame()
vif['Features'] = VIF_compatible.columns
vif['VIF'] = [variance_inflation_factor(VIF_compatible.values, i) for i in range(VIF_compatible.shape[1])]
# Display the VIF values for each feature
print(vif)

VIF_compatible.drop(columns = ['Infant_mortality'], inplace = True)

# Compute VIF values
vif = pd.DataFrame()
vif['Features'] = VIF_compatible.columns
vif['VIF'] = [variance_inflation_factor(VIF_compatible.values, i) for i in range(VIF_compatible.shape[1])]
# Display the VIF values for each feature
print(vif)

"""Although BMI still has a VIF value greater than 100, we can get away with this since CART models are generally less sensitive to multicollinearity. I also want to keep the BMI variable since it seems like it would be a very relevant feature for determining life expectancy.

Creating a csv that drops columns from the previous VIF calculations and saving 'Year' because we are using year to condition training and test data
"""

csv_cart = merged_output_cart.drop(columns = ['Infant_mortality','Country', 'Status',' thinness  1-19 years', 'non_communicable_disease', 'infant deaths'])
csv_cart.head()

#Create training and test data based on year. Training:2000-2012, Test:2013-2015
train_cart = csv_cart[(csv_cart['Year'] >= 2000) & (csv_cart['Year'] <= 2012)]
test_cart = csv_cart[(csv_cart['Year'] >= 2013) & (csv_cart['Year'] <= 2015)]

#drop the year column for the cart model
train_noyear = train_cart.drop(columns = ['Year'])
test_noyear = test_cart.drop(columns = ['Year'])

train_noyear.describe()

"""We can see from above that there are extreme outliers for some of the variables. For example, the 75th percentile of alcohol is 7.91 but the max is 1766.95. This is not unique to just the alcohol variable so we use the winsorize function below to replace the bottom and top 2.5th percentiles with the 2.5th and 97.5th percentile respectively."""

for column in train_noyear.columns:
    train_noyear[column] = winsorize(train_noyear[column], limits=[0.025, 0.025])

for column in test_noyear.columns:
    test_noyear[column] = winsorize(test_noyear[column], limits=[0.025, 0.025])

X_train_cart = train_noyear.drop(columns = 'Life expectancy ')
y_train_cart = train_noyear['Life expectancy ']

X_test_cart = test_noyear.drop(columns = 'Life expectancy ')
y_test_cart = test_noyear['Life expectancy ']

"""We started tuning alpha over the range (0,.5) and got that the best alpha was .01515 so we narrowed down the range to (0,.15). This resulted in the best alpha being .01667. The final range of alpha we used was (0,1) which resulted in the best alpha being = .01616"""

# Create a grid of possible ccp_alpha values
param_grid = {'ccp_alpha': np.linspace(0, 0.1, 100)}

# Initialize the regression tree model
regressor = DecisionTreeRegressor(random_state=42)

# Set up cross-validation using KFolds
cross_validation = KFold(n_splits=10, random_state=42, shuffle=True)

# Initialize GridSearchCV
regressor_cv = GridSearchCV(regressor, param_grid=param_grid, scoring='r2', cv=cross_validation, verbose=0)

# Fit the GridSearchCV
regressor_cv.fit(X_train_cart, y_train_cart)

# Print the best ccp_alpha value found
print("Best ccp_alpha:", regressor_cv.best_params_)

# Train the model with the best parameters
best_regressor = DecisionTreeRegressor(ccp_alpha=regressor_cv.best_params_['ccp_alpha'], random_state=42)
best_regressor.fit(X_train_cart, y_train_cart)

# Make predictions on the test data
y_pred_cart = best_regressor.predict(X_test_cart)

# Mean Squared Error (MSE)
mse = mean_squared_error(y_test_cart, y_pred_cart)
print("Mean Squared Error:", mse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test_cart, y_pred_cart)
print("Mean Absolute Error:", mae)

# R-squared score
r2 = r2_score(y_test_cart, y_pred_cart)
print("R-squared score:", r2)

"""Accuracy is generally not used to evaluate regression tree models so the part below isn't really necessary and can be ignored or removed for the final deliverable.The high R-squared and ok MSE values above indicate this model that the regression tree model is doing an pretty decent job of capturing the overall relationship between the predictor variables and the target variable."""

y_pred_regression = best_regressor.predict(X_test_cart)
y_pred_regression_rounded = [round(x) for x in y_pred_regression]

y_test_rounded = [round(x) for x in y_test_cart]

accuracy_cart = accuracy_score(y_test_rounded, y_pred_regression_rounded)
accuracy_percent = accuracy_cart * 100

# Print the accuracy score as a percentage
print("Accuracy: {:.2f}%".format(accuracy_percent))

for_plot = DecisionTreeRegressor(max_leaf_nodes = 20, ccp_alpha=regressor_cv.best_params_['ccp_alpha'], random_state=42)
for_plot.fit(X_train_cart, y_train_cart)

plt.figure(figsize=(20, 10))

plot_tree(for_plot, feature_names=X_train.columns, rounded = True, filled=True, fontsize=6)

plt.show()

"""## 11. Random Forest - Bagging"""

def OSR2(model, X_test, y_test, y_train):

    y_pred = model.predict(X_test)
    SSE = np.sum((y_test - y_pred)**2)
    SST = np.sum((y_test - np.mean(y_train))**2)

    return (1 - SSE/SST)



#Train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=88)
X_train.shape, X_test.shape

#Random Forest - Bagging

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(max_features=5, min_samples_leaf=5,
                           n_estimators = 500, random_state=88, verbose=2)

rf.fit(X_train, y_train)

rf.verbose = False

print('OSR2:', round(OSR2(rf, X_test, y_test, y_train), 5))

#Feature Importance of all columns
pd.DataFrame({'Feature' : X_train.columns,
              'Importance score': 100*rf.feature_importances_}).round(1).sort_values(by='Importance score', ascending=False)

"""We can see that Adult Mortalilty, BMI, thiness and income are consdiered as important feature due to high importance score but we need to find how many feautures are we gonna use. So we use GridSearchCV to find best max_features"""

## using GridSearchCV to find best max_features: (this will take awhile)

import time

grid_values = {'max_features': [5, 6, 7],
               'min_samples_leaf': [5],
               'n_estimators': [100, 200, 300, 400, 500],
               'random_state': [88]}

tic = time.time()

rf2 = RandomForestRegressor()

cv = KFold(n_splits=5,random_state=333,shuffle=True)
rf_cv = GridSearchCV(rf2, param_grid=grid_values, scoring='r2', cv=cv,verbose=2)
rf_cv.fit(X_train, y_train)

toc = time.time()

print('time:', round(toc-tic, 2),'s')

#select the best hyperparameter
max_features = rf_cv.cv_results_['param_max_features'].data
R2_scores = rf_cv.cv_results_['mean_test_score']
pd.DataFrame({'Max feature' : max_features,
              'Mean test score': R2_scores }).round(1)

print(rf_cv.best_params_)

print('Cross-validated R2:', round(rf_cv.best_score_, 5))
print('OSR2:', round(OSR2(rf_cv, X_test, y_test, y_train), 5))

#updated rf with max_features = 6
rf2 = RandomForestRegressor(max_features=6, min_samples_leaf=5,
                           n_estimators = 500, random_state=88, verbose=2)

rf2.fit(X_train,y_train)

from sklearn.metrics import mean_squared_error

# Make predictions using the RandomForestRegressor
y_pred = rf2.predict(X_test)

# Calculate the mean squared error (MSE)
mse = mean_squared_error(y_test, y_pred)

print(f"Mean squared error: {mse:.2f}")

from sklearn.metrics import mean_absolute_error

# Make predictions using the RandomForestRegressor
y_pred = rf2.predict(X_test)

# Calculate the mean absolute error (MAE)
mae = mean_absolute_error(y_test, y_pred)

print(f"Mean absolute error: {mae:.2f}")

"""#conclusion
Based on the graph and analysis, having negative R2 and low OSR2 indicates that Bagging is not a good model.
It shows that the model is not able to explain any of the variance in the dependent variable and is performing worse than the mean of the dependent variable in making predictions.
In this case, it is usually better to use a simpler model or to collect more data to improve the performance of the model.

"""